{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "from torchvision import transforms\n",
    "# from dataset.data_loader import GetLoader\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MNIST(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\" Intialize the MNIST dataset \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "                              \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset \"\"\"\n",
    "        img_name = os.path.join(self.root_dir,self.landmarks_frame.iloc[index,0])\n",
    "        image = io.imread(img_name)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        label = self.landmarks_frame['label'][index]\n",
    "        label = torch.FloatTensor([label])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Total number of samples in the dataset \"\"\"\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "def test(dataset_name, epoch):\n",
    "    assert dataset_name in ['MNIST', 'SVHN']\n",
    "\n",
    "    model_root = os.path.join('.','model')\n",
    "    image_root = os.path.join('..', 'dataset', dataset_name)\n",
    "\n",
    "    cuda = True\n",
    "    cudnn.benchmark = True\n",
    "    batch_size = 128\n",
    "    image_size = 28\n",
    "    alpha = 0\n",
    "\n",
    "    \"\"\"load data\"\"\"\n",
    "\n",
    "    # img_transform_source = transforms.Compose([\n",
    "    #     transforms.Resize(image_size),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    # ])\n",
    "\n",
    "    # img_transform_target = transforms.Compose([\n",
    "    #     transforms.Resize(image_size),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    # ])\n",
    "\n",
    "    if dataset_name == 'SVHN':\n",
    "        # test_list = os.path.join(image_root, 'mnist_m_test_labels.txt')\n",
    "\n",
    "        # dataset = GetLoader(\n",
    "        #     data_root=os.path.join(image_root, 'mnist_m_test'),\n",
    "        #     data_list=test_list,\n",
    "        #     transform=img_transform_target\n",
    "        # )\n",
    "        dataset =  MNIST(csv_file=\"../../hw3_data/digits/svhn/test.csv\", root_dir=\"../../hw3_data/digits/svhn/test\",transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "    else:\n",
    "        dataset = MNIST(csv_file=\"../../hw3_data/digits/mnistm/test.csv\", root_dir=\"../../hw3_data/digits/mnistm/test\",transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8\n",
    "    )\n",
    "\n",
    "    \"\"\" training \"\"\"\n",
    "\n",
    "    my_net = torch.load(os.path.join(\n",
    "        model_root, 'mnist_svhn_model' + str(epoch) + '.pth'\n",
    "    ))\n",
    "    my_net = my_net.eval()\n",
    "\n",
    "    if cuda:\n",
    "        my_net = my_net.cuda()\n",
    "\n",
    "    len_dataloader = len(dataloader)\n",
    "    data_target_iter = iter(dataloader)\n",
    "\n",
    "    i = 0\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "#     features = []\n",
    "    while i < len_dataloader:\n",
    "\n",
    "        # test model using target data\n",
    "        data_target = data_target_iter.next()\n",
    "        t_img, t_label = data_target\n",
    "\n",
    "        batch_size = len(t_label)\n",
    "\n",
    "        input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
    "        class_label = torch.LongTensor(batch_size)\n",
    "\n",
    "        if cuda:\n",
    "            t_img = t_img.cuda()\n",
    "            t_label = t_label.cuda()\n",
    "            input_img = input_img.cuda()\n",
    "            class_label = class_label.cuda()\n",
    "\n",
    "        input_img.resize_as_(t_img).copy_(t_img)\n",
    "        class_label.resize_as_(t_label.long()).copy_(t_label.long())\n",
    "\n",
    "        class_output, _,_ = my_net(input_data=input_img, alpha=alpha)\n",
    "#         features.append(feature.cpu())\n",
    "        pred = class_output.data.max(1, keepdim=True)[1]\n",
    "        n_correct += pred.eq(class_label.data.view_as(pred)).cpu().sum()\n",
    "        n_total += batch_size\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    accu = n_correct.data.numpy() * 1.0 / n_total\n",
    "\n",
    "    print ('epoch: %d, accuracy of the %s dataset: %f' % (epoch, dataset_name, accu))\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy of the SVHN dataset: 0.451444\n",
      "epoch: 1, accuracy of the SVHN dataset: 0.453634\n",
      "epoch: 2, accuracy of the SVHN dataset: 0.436540\n",
      "epoch: 3, accuracy of the SVHN dataset: 0.436616\n",
      "epoch: 4, accuracy of the SVHN dataset: 0.456592\n",
      "epoch: 5, accuracy of the SVHN dataset: 0.448832\n",
      "epoch: 6, accuracy of the SVHN dataset: 0.454364\n",
      "epoch: 7, accuracy of the SVHN dataset: 0.473725\n",
      "epoch: 8, accuracy of the SVHN dataset: 0.481446\n",
      "epoch: 9, accuracy of the SVHN dataset: 0.474493\n",
      "epoch: 10, accuracy of the SVHN dataset: 0.482253\n",
      "epoch: 11, accuracy of the SVHN dataset: 0.481484\n",
      "epoch: 12, accuracy of the SVHN dataset: 0.476490\n",
      "epoch: 13, accuracy of the SVHN dataset: 0.445337\n",
      "epoch: 14, accuracy of the SVHN dataset: 0.471766\n",
      "epoch: 15, accuracy of the SVHN dataset: 0.494968\n",
      "epoch: 16, accuracy of the SVHN dataset: 0.494199\n"
     ]
    }
   ],
   "source": [
    "for i in range(17):\n",
    "    test(\"SVHN\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "from torchvision import transforms\n",
    "# from dataset.data_loader import GetLoader\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MNIST(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\" Intialize the MNIST dataset \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "                              \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset \"\"\"\n",
    "        img_name = os.path.join(self.root_dir,self.landmarks_frame.iloc[index,0])\n",
    "        image = io.imread(img_name)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        label = self.landmarks_frame['label'][index]\n",
    "        label = torch.FloatTensor([label])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Total number of samples in the dataset \"\"\"\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "class USPS(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\" Intialize the MNIST dataset \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "                              \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset \"\"\"\n",
    "        img_name = os.path.join(self.root_dir,self.landmarks_frame.iloc[index,0])\n",
    "        image = io.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        label = self.landmarks_frame['label'][index]\n",
    "        label = torch.FloatTensor([label])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Total number of samples in the dataset \"\"\"\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "def test(dataset_name, epoch):\n",
    "    assert dataset_name in ['USPS', 'MNIST']\n",
    "\n",
    "    model_root = os.path.join('.','model')\n",
    "    image_root = os.path.join('..', 'dataset', dataset_name)\n",
    "\n",
    "    cuda = True\n",
    "    cudnn.benchmark = True\n",
    "    batch_size = 128\n",
    "    image_size = 28\n",
    "    alpha = 0\n",
    "\n",
    "    \"\"\"load data\"\"\"\n",
    "\n",
    "    # img_transform_source = transforms.Compose([\n",
    "    #     transforms.Resize(image_size),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    # ])\n",
    "\n",
    "    # img_transform_target = transforms.Compose([\n",
    "    #     transforms.Resize(image_size),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    # ])\n",
    "\n",
    "    if dataset_name == 'MNIST':\n",
    "        # test_list = os.path.join(image_root, 'mnist_m_test_labels.txt')\n",
    "\n",
    "        # dataset = GetLoader(\n",
    "        #     data_root=os.path.join(image_root, 'mnist_m_test'),\n",
    "        #     data_list=test_list,\n",
    "        #     transform=img_transform_target\n",
    "        # )\n",
    "        dataset =  MNIST(csv_file=\"../../hw3_data/digits/mnistm/test.csv\", root_dir=\"../../hw3_data/digits/mnistm/test\",transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "    else:\n",
    "        dataset = USPS(csv_file=\"../../hw3_data/digits/usps/test.csv\", root_dir=\"../../hw3_data/digits/usps/test\",transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8\n",
    "    )\n",
    "\n",
    "    \"\"\" training \"\"\"\n",
    "\n",
    "    my_net = torch.load(os.path.join(\n",
    "        model_root, 'usps_mnist_model' + str(epoch) + '.pth'\n",
    "    ))\n",
    "    my_net = my_net.eval()\n",
    "\n",
    "    if cuda:\n",
    "        my_net = my_net.cuda()\n",
    "\n",
    "    len_dataloader = len(dataloader)\n",
    "    data_target_iter = iter(dataloader)\n",
    "\n",
    "    i = 0\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    while i < len_dataloader:\n",
    "\n",
    "        # test model using target data\n",
    "        data_target = data_target_iter.next()\n",
    "        t_img, t_label = data_target\n",
    "\n",
    "        batch_size = len(t_label)\n",
    "\n",
    "        input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
    "        class_label = torch.LongTensor(batch_size)\n",
    "\n",
    "        if cuda:\n",
    "            t_img = t_img.cuda()\n",
    "            t_label = t_label.cuda()\n",
    "            input_img = input_img.cuda()\n",
    "            class_label = class_label.cuda()\n",
    "\n",
    "        input_img.resize_as_(t_img).copy_(t_img)\n",
    "        class_label.resize_as_(t_label.long()).copy_(t_label.long())\n",
    "\n",
    "        class_output, _,_ = my_net(input_data=input_img, alpha=alpha)\n",
    "        pred = class_output.data.max(1, keepdim=True)[1]\n",
    "        n_correct += pred.eq(class_label.data.view_as(pred)).cpu().sum()\n",
    "        n_total += batch_size\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    accu = n_correct.data.numpy() * 1.0 / n_total\n",
    "\n",
    "    print ('epoch: %d, accuracy of the %s dataset: %f' % (epoch, dataset_name, accu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robot/anaconda3/envs/env/lib/python3.7/site-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'model.CNNModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy of the MNIST dataset: 0.304300\n",
      "epoch: 1, accuracy of the MNIST dataset: 0.361600\n",
      "epoch: 2, accuracy of the MNIST dataset: 0.348500\n",
      "epoch: 3, accuracy of the MNIST dataset: 0.367800\n",
      "epoch: 4, accuracy of the MNIST dataset: 0.384300\n",
      "epoch: 5, accuracy of the MNIST dataset: 0.388300\n",
      "epoch: 6, accuracy of the MNIST dataset: 0.377500\n",
      "epoch: 7, accuracy of the MNIST dataset: 0.408700\n",
      "epoch: 8, accuracy of the MNIST dataset: 0.387300\n",
      "epoch: 9, accuracy of the MNIST dataset: 0.412800\n",
      "epoch: 10, accuracy of the MNIST dataset: 0.414500\n",
      "epoch: 11, accuracy of the MNIST dataset: 0.396600\n",
      "epoch: 12, accuracy of the MNIST dataset: 0.377900\n",
      "epoch: 13, accuracy of the MNIST dataset: 0.407300\n",
      "epoch: 14, accuracy of the MNIST dataset: 0.406500\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    test(\"MNIST\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "from torchvision import transforms\n",
    "# from dataset.data_loader import GetLoader\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MNIST(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\" Intialize the MNIST dataset \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "                              \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset \"\"\"\n",
    "        img_name = os.path.join(self.root_dir,self.landmarks_frame.iloc[index,0])\n",
    "        image = io.imread(img_name)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        label = self.landmarks_frame['label'][index]\n",
    "        label = torch.FloatTensor([label])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Total number of samples in the dataset \"\"\"\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "class USPS(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\" Intialize the MNIST dataset \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "                              \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset \"\"\"\n",
    "        img_name = os.path.join(self.root_dir,self.landmarks_frame.iloc[index,0])\n",
    "        image = io.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        label = self.landmarks_frame['label'][index]\n",
    "        label = torch.FloatTensor([label])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Total number of samples in the dataset \"\"\"\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "def test(dataset_name, epoch):\n",
    "    assert dataset_name in ['SVHN', 'USPS']\n",
    "\n",
    "    model_root = os.path.join('.','model')\n",
    "    image_root = os.path.join('..', 'dataset', dataset_name)\n",
    "\n",
    "    cuda = True\n",
    "    cudnn.benchmark = True\n",
    "    batch_size = 128\n",
    "    image_size = 28\n",
    "    alpha = 0\n",
    "\n",
    "    \"\"\"load data\"\"\"\n",
    "\n",
    "    # img_transform_source = transforms.Compose([\n",
    "    #     transforms.Resize(image_size),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    # ])\n",
    "\n",
    "    # img_transform_target = transforms.Compose([\n",
    "    #     transforms.Resize(image_size),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    # ])\n",
    "\n",
    "    if dataset_name == 'USPS':\n",
    "        # test_list = os.path.join(image_root, 'mnist_m_test_labels.txt')\n",
    "\n",
    "        # dataset = GetLoader(\n",
    "        #     data_root=os.path.join(image_root, 'mnist_m_test'),\n",
    "        #     data_list=test_list,\n",
    "        #     transform=img_transform_target\n",
    "        # )\n",
    "        dataset =  USPS(csv_file=\"../../hw3_data/digits/usps/test.csv\", root_dir=\"../../hw3_data/digits/usps/test\",transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "    else:\n",
    "        dataset = MNIST(csv_file=\"../../hw3_data/digits/svhn/test.csv\", root_dir=\"../../hw3_data/digits/svhn/test\",transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8\n",
    "    )\n",
    "\n",
    "    \"\"\" training \"\"\"\n",
    "\n",
    "    my_net = torch.load(os.path.join(\n",
    "        model_root, 'svhn_usps_model' + str(epoch) + '.pth'\n",
    "    ))\n",
    "    my_net = my_net.eval()\n",
    "\n",
    "    if cuda:\n",
    "        my_net = my_net.cuda()\n",
    "\n",
    "    len_dataloader = len(dataloader)\n",
    "    data_target_iter = iter(dataloader)\n",
    "\n",
    "    i = 0\n",
    "    n_total = 0\n",
    "    n_correct = 0\n",
    "\n",
    "    while i < len_dataloader:\n",
    "\n",
    "        # test model using target data\n",
    "        data_target = data_target_iter.next()\n",
    "        t_img, t_label = data_target\n",
    "\n",
    "        batch_size = len(t_label)\n",
    "\n",
    "        input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
    "        class_label = torch.LongTensor(batch_size)\n",
    "\n",
    "        if cuda:\n",
    "            t_img = t_img.cuda()\n",
    "            t_label = t_label.cuda()\n",
    "            input_img = input_img.cuda()\n",
    "            class_label = class_label.cuda()\n",
    "\n",
    "        input_img.resize_as_(t_img).copy_(t_img)\n",
    "        class_label.resize_as_(t_label.long()).copy_(t_label.long())\n",
    "\n",
    "        class_output, _,_ = my_net(input_data=input_img, alpha=alpha)\n",
    "        pred = class_output.data.max(1, keepdim=True)[1]\n",
    "        n_correct += pred.eq(class_label.data.view_as(pred)).cpu().sum()\n",
    "        n_total += batch_size\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    accu = n_correct.data.numpy() * 1.0 / n_total\n",
    "\n",
    "    print ('epoch: %d, accuracy of the %s dataset: %f' % (epoch, dataset_name, accu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy of the USPS dataset: 0.257598\n",
      "epoch: 1, accuracy of the USPS dataset: 0.343298\n",
      "epoch: 2, accuracy of the USPS dataset: 0.470852\n",
      "epoch: 3, accuracy of the USPS dataset: 0.560538\n",
      "epoch: 4, accuracy of the USPS dataset: 0.565521\n",
      "epoch: 5, accuracy of the USPS dataset: 0.605879\n",
      "epoch: 6, accuracy of the USPS dataset: 0.574988\n",
      "epoch: 7, accuracy of the USPS dataset: 0.514200\n",
      "epoch: 8, accuracy of the USPS dataset: 0.558545\n",
      "epoch: 9, accuracy of the USPS dataset: 0.520678\n",
      "epoch: 10, accuracy of the USPS dataset: 0.516692\n",
      "epoch: 11, accuracy of the USPS dataset: 0.431988\n",
      "epoch: 12, accuracy of the USPS dataset: 0.404086\n",
      "epoch: 13, accuracy of the USPS dataset: 0.338316\n",
      "epoch: 14, accuracy of the USPS dataset: 0.497758\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    test(\"USPS\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, accuracy of the SVHN dataset: 0.482253\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "from torchvision import transforms\n",
    "# from dataset.data_loader import GetLoader\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MNIST(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\" Intialize the MNIST dataset \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "                              \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset \"\"\"\n",
    "        img_name = os.path.join(self.root_dir,self.landmarks_frame.iloc[index,0])\n",
    "        image = io.imread(img_name)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        label = self.landmarks_frame['label'][index]\n",
    "        label = torch.FloatTensor([label])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Total number of samples in the dataset \"\"\"\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "# def test(dataset_name, epoch):\n",
    "dataset_name = 'SVHN'\n",
    "epoch = 10\n",
    "assert dataset_name in ['MNIST', 'SVHN']\n",
    "\n",
    "model_root = os.path.join('.','model')\n",
    "image_root = os.path.join('..', 'dataset', dataset_name)\n",
    "\n",
    "cuda = True\n",
    "cudnn.benchmark = True\n",
    "batch_size = 128\n",
    "image_size = 28\n",
    "alpha = 0\n",
    "\n",
    "\"\"\"load data\"\"\"\n",
    "\n",
    "# img_transform_source = transforms.Compose([\n",
    "#     transforms.Resize(image_size),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "# ])\n",
    "\n",
    "# img_transform_target = transforms.Compose([\n",
    "#     transforms.Resize(image_size),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "if dataset_name == 'SVHN':\n",
    "    # test_list = os.path.join(image_root, 'mnist_m_test_labels.txt')\n",
    "\n",
    "    # dataset = GetLoader(\n",
    "    #     data_root=os.path.join(image_root, 'mnist_m_test'),\n",
    "    #     data_list=test_list,\n",
    "    #     transform=img_transform_target\n",
    "    # )\n",
    "    dataset =  MNIST(csv_file=\"../../hw3_data/digits/svhn/test.csv\", root_dir=\"../../hw3_data/digits/svhn/test\",transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                       ]))\n",
    "else:\n",
    "    dataset = MNIST(csv_file=\"../../hw3_data/digits/mnistm/test.csv\", root_dir=\"../../hw3_data/digits/mnistm/test\",transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                       ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "\"\"\" training \"\"\"\n",
    "\n",
    "my_net = torch.load(os.path.join(\n",
    "    model_root, 'mnist_svhn_model' + str(epoch) + '.pth'\n",
    "))\n",
    "my_net = my_net.eval()\n",
    "\n",
    "if cuda:\n",
    "    my_net = my_net.cuda()\n",
    "\n",
    "len_dataloader = len(dataloader)\n",
    "data_target_iter = iter(dataloader)\n",
    "\n",
    "i = 0\n",
    "n_total = 0\n",
    "n_correct = 0\n",
    "predict = []\n",
    "while i < len_dataloader:\n",
    "\n",
    "    # test model using target data\n",
    "    data_target = data_target_iter.next()\n",
    "    t_img, t_label = data_target\n",
    "\n",
    "    batch_size = len(t_label)\n",
    "\n",
    "    input_img = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
    "    class_label = torch.LongTensor(batch_size)\n",
    "\n",
    "    if cuda:\n",
    "        t_img = t_img.cuda()\n",
    "        t_label = t_label.cuda()\n",
    "        input_img = input_img.cuda()\n",
    "        class_label = class_label.cuda()\n",
    "\n",
    "    input_img.resize_as_(t_img).copy_(t_img)\n",
    "    class_label.resize_as_(t_label.long()).copy_(t_label.long())\n",
    "\n",
    "    class_output, _,_ = my_net(input_data=input_img, alpha=alpha)\n",
    "#         features.append(feature.cpu())\n",
    "    pred = class_output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "    n_correct += pred.eq(class_label.data.view_as(pred)).cpu().sum()\n",
    "    predict += pred.squeeze().cpu().tolist()\n",
    "    n_total += batch_size\n",
    "\n",
    "    i += 1\n",
    "\n",
    "accu = n_correct.data.numpy() * 1.0 / n_total\n",
    "\n",
    "print ('epoch: %d, accuracy of the %s dataset: %f' % (epoch, dataset_name, accu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 8,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [7],\n",
       "        [5],\n",
       "        [2],\n",
       "        [4],\n",
       "        [5],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [5],\n",
       "        [1],\n",
       "        [8],\n",
       "        [2],\n",
       "        [9],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [6],\n",
       "        [4],\n",
       "        [7],\n",
       "        [8],\n",
       "        [8],\n",
       "        [1],\n",
       "        [5],\n",
       "        [9],\n",
       "        [7],\n",
       "        [0],\n",
       "        [1],\n",
       "        [8],\n",
       "        [8],\n",
       "        [3],\n",
       "        [4],\n",
       "        [2],\n",
       "        [9],\n",
       "        [8],\n",
       "        [1],\n",
       "        [5],\n",
       "        [1],\n",
       "        [7],\n",
       "        [1],\n",
       "        [4],\n",
       "        [8],\n",
       "        [8],\n",
       "        [2],\n",
       "        [7],\n",
       "        [0],\n",
       "        [1]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
